<?xml version="1.0" encoding="utf-8"?>
<transcoder>
 <workflow nameConvention="[path][baseName][.frame][.ext]" name="wf_footage_process">
  <view y="-568.69123075394657" x="-136.60215253203828" scale="1.0321945693127366"/>
  <nodes>
   <node class="Watcher" type="input">
    <properties>
     <objectName type="string">Watchfolder</objectName>
     <color type="color">#aa557f</color>
     <colorByStatus type="bool">false</colorByStatus>
     <pos type="point2">-926.988,-676.124</pos>
     <schemaName type="string">watcher</schemaName>
     <nameConvention type="CnameConvention">[path][baseName][.frame][.ext]</nameConvention>
     <nodeLocked type="bool">false</nodeLocked>
     <bypassSupported type="bool">true</bypassSupported>
     <bypassEnabled type="bool">false</bypassEnabled>
     <renderFarmSupported type="bool">false</renderFarmSupported>
     <renderFarmSplitContent type="bool">false</renderFarmSplitContent>
     <farmParams type="string"></farmParams>
     <auxCode type="string"></auxCode>
     <renderFarmEnabled type="bool">false</renderFarmEnabled>
     <url type="string">/Volumes/konsistent/server/mistika-testing/INPUT/</url>
     <autoUpdateNameConvention type="bool">false</autoUpdateNameConvention>
     <filterMode type="int">1</filterMode>
     <fileNameOnly type="bool">true</fileNameOnly>
     <include type="string"></include>
     <exclude type="string"></exclude>
     <latency type="int">5</latency>
     <forceCheck type="int">0</forceCheck>
     <recursive type="bool">true</recursive>
     <addRoot type="bool">true</addRoot>
     <deleteAfterProcessing type="bool">false</deleteAfterProcessing>
     <areYouSure type="bool">true</areYouSure>
     <maintainPadding type="bool">true</maintainPadding>
     <alphabeticalOrder type="bool">false</alphabeticalOrder>
     <continuePreviousExecutions type="bool">false</continuePreviousExecutions>
     <numberOfTries type="int">5</numberOfTries>
     <globalTimer type="bool">true</globalTimer>
    </properties>
    <connections>
     <rankFrom id="781edd60-bc29-4d51-bf1f-78e130faf861" type="input" name="rankFrom" specialType="order"/>
     <rankTo id="b3a49d3f-afef-4ee0-bf53-b3898a1dce49" type="output" name="rankTo" specialType="order"/>
     <To id="483b133f-9b78-4a50-b730-03f2417ee591" type="output" name="To">
      <linkedTo>11a77302-3841-4dc6-b7b5-d1afce2d9ba0</linkedTo>
     </To>
    </connections>
   </node>
   <node class="jsonToTokens" type="task">
    <properties>
     <objectName type="string">jsonToTokens</objectName>
     <color type="color">#01376b</color>
     <colorByStatus type="bool">false</colorByStatus>
     <pos type="point2">-590.546,-677.819</pos>
     <schemaName type="string">jsontotokens</schemaName>
     <nameConvention type="CnameConvention"></nameConvention>
     <nodeLocked type="bool">false</nodeLocked>
     <bypassSupported type="bool">true</bypassSupported>
     <bypassEnabled type="bool">false</bypassEnabled>
     <renderFarmSupported type="bool">false</renderFarmSupported>
     <renderFarmSplitContent type="bool">false</renderFarmSplitContent>
     <farmParams type="string"></farmParams>
     <auxCode type="string"></auxCode>
     <renderFarmEnabled type="bool">false</renderFarmEnabled>
     <code type="string"># File lives here: /Users/[username]/SGO AppData/shared/workflowsLibrary/

from Mistika.Qt import QColor
from Mistika.classes import Cconnector
from pathlib import Path
import json

from token2mdata import token2mdataMapper

################################################################################
# Json to Dynamic Tokens Mistika Node                                          #
# ===================================                                          #
# E.Spencer - Konsistent Consulting 2025                                       #
# Takes a given .json file and converts the values into dynamic tokens within  #
# mistika                                                                      #
################################################################################

REQUIRED_KEYS = ("brand", "campaign", "job", "s3_path")

def _info(self, msg):
    try:
        self.info("jsonToTokens", msg, "")
    except Exception:
        pass


def _critical(self, msg_id, msg):
    return self.critical(msg_id, msg, "")


def _load_json(self):
    """
    Load the JSON file from self.jsonFile.
    Always try to read it, regardless of UPs.
    """
    raw = (self.jsonFile or "").strip()
    if not raw:
        _critical(self, "jsonToTokens:jsonFileEmpty", "jsonFile property is empty")
        return None

    try:
        path_str = self.evaluate(raw).strip()
    except Exception:
        path_str = raw

    json_path = Path(path_str).expanduser()

    if not json_path.exists() or not json_path.is_file():
        _critical(self, "jsonToTokens:jsonMissing", f"JSON file not found: {json_path}")
        return None

    try:
        with json_path.open("r", encoding="utf-8") as f:
            data = json.load(f)
    except Exception as e:
        _critical(self, "jsonToTokens:jsonParse", f"Failed to parse JSON '{json_path}': {e}")
        return None

    _info(self, f"Using JSON file: {json_path}")
    return data


def _extract_values(self, data):
    """
    Extract brand, campaign, job from the JSON root.
    Log exactly ONE line per value.
    """
    if not isinstance(data, dict):
        _critical(self, "jsonToTokens:jsonShape",
                  "JSON root must be an object with brand/campaign/job keys")
        return None

    values = {}
    missing = []
    for key in REQUIRED_KEYS:
        v = data.get(key)
        if v is None or (isinstance(v, str) and v.strip() == ""):
            missing.append(key)
        else:
            values[key] = str(v)

    if missing:
        _critical(self, "jsonToTokens:jsonMissingKeys",
                  "JSON is missing required keys: " + ", ".join(missing))
        return None

    _info(self, f"brand   (JSON) = {values['brand']}")
    _info(self, f"campaign(JSON) = {values['campaign']}")
    _info(self, f"job     (JSON) = {values['job']}")
    _info(self, f"s3_path (JSON) = {values['s3_path']}")

    return values


def _apply_to_up(self, mapper, up, values):
    """
    Attach brand/campaign/job to the UP and then let token2mdataMapper
    sync into metadata / NC the Mistika way.

    Also logs the values *after* assignment to confirm they are set.
    """
    for key in REQUIRED_KEYS:
        val = values.get(key, "")
        try:
            up.setParam(key, val)
            _info(self, f"setParam[{key}] = {val}")
        except Exception as e:
            _info(self, f"setParam[{key}] failed: {e}")

    try:
        new_up = mapper.nc2mdata(up)
        if new_up is not None:
            up = new_up
        _info(self, "token2mdataMapper.nc2mdata() applied")
    except Exception as e:
        _info(self, f"token2mdataMapper.nc2mdata failed (non-fatal): {e}")

    try:
        up.updatePlaceHolders(True)
    except Exception:
        pass

    for key in REQUIRED_KEYS:
        try:
            val_after = up.getParam(key, "")
        except Exception:
            val_after = ""
        _info(self, f"CONFIRM token[{key}] = '{val_after}'")

    return up


def init(self):
    self.setClassName("jsonToTokens")
    self.color = QColor(0x01, 0x37, 0x6B)

    self.addConnector("input",  Cconnector.CONNECTOR_TYPE_INPUT,  Cconnector.MODE_OPTIONAL)
    self.addConnector("output", Cconnector.CONNECTOR_TYPE_OUTPUT, Cconnector.MODE_OPTIONAL)
    self.setAcceptConnectors(True, "input")

    self.addProperty("jsonFile", "")
    self.addProperty("objectName", "")

    self.bypassSupported = True
    self.setSupportedTypes(self.NODETYPE_TASK)
    self.setComplexity(10)
    return True


def isReady(self):
    """
    Simple check: jsonFile must be set (existence re-checked in process).
    """
    if self.bypassSupported and self.bypassEnabled:
        return True

    if not (self.jsonFile or "").strip():
        return _critical(self, "jsonToTokens:jsonFileEmpty",
                         "'jsonFile' must point to a JSON file")
    return True


def process(self):
    res = True

    inputs = self.getConnectorsByType(Cconnector.CONNECTOR_TYPE_INPUT)
    output = self.getFirstConnectorByName("output")
    if output:
        output.clearUniversalPaths()

    if self.bypassSupported and self.bypassEnabled:
        self.progressUpdated(self.complexity())
        if output:
            for c in inputs:
                for up in c.getUniversalPaths():
                    output.addUniversalPath(up)
        return True

    data = _load_json(self)
    if data is None:
        return False

    values = _extract_values(self, data)
    if values is None:
        return False

    ups_in = []
    for c in inputs:
        ups_in.extend(c.getUniversalPaths())

    if not ups_in:
        return True

    try:
        mapper = token2mdataMapper(self)
    except Exception as e:
        return _critical(self, "jsonToTokens:mapper",
                         f"Failed to create token2mdataMapper: {e}")

    for up in ups_in:
        if self.isCancelled():
            return False
        new_up = _apply_to_up(self, mapper, up, values)
        if output:
            output.addUniversalPath(new_up)

    return res


def onPropertyUpdated(self, name):
    try:
        pass
    except AttributeError:
        pass
</code>
     <jsonFile type="string">/Volumes/konsistent/server/mistika-testing/json/sample.json</jsonFile>
    </properties>
    <connections>
     <rankFrom id="e871caad-c5db-4ee5-8c74-8de6e196d463" type="input" name="rankFrom" specialType="order"/>
     <rankTo id="5a60a7b5-f288-48d1-9af0-7a38a582c837" type="output" name="rankTo" specialType="order"/>
     <input id="11a77302-3841-4dc6-b7b5-d1afce2d9ba0" type="input" name="input">
      <linkedTo>483b133f-9b78-4a50-b730-03f2417ee591</linkedTo>
     </input>
     <output id="e0ae5b88-2d5f-4091-8f17-555e27d0956e" type="output" name="output">
      <linkedTo>edf578e0-3b21-4c8e-ab01-501a0096c864</linkedTo>
      <linkedTo>9a571c3e-f5e1-451e-b066-881408ad2b28</linkedTo>
      <linkedTo>8a2f21e4-543c-4a90-adc7-dc6d0e7f3352</linkedTo>
     </output>
    </connections>
   </node>
   <node class="ProRes" type="task">
    <properties>
     <objectName type="string">ConvertToProres</objectName>
     <color type="color">#0063b4</color>
     <colorByStatus type="bool">false</colorByStatus>
     <pos type="point2">-258.514,-658.236</pos>
     <schemaName type="string">prores</schemaName>
     <nameConvention type="CnameConvention"></nameConvention>
     <nodeLocked type="bool">false</nodeLocked>
     <bypassSupported type="bool">true</bypassSupported>
     <bypassEnabled type="bool">false</bypassEnabled>
     <renderFarmSupported type="bool">true</renderFarmSupported>
     <renderFarmSplitContent type="bool">false</renderFarmSplitContent>
     <farmParams type="string"></farmParams>
     <auxCode type="string"></auxCode>
     <renderFarmEnabled type="bool">false</renderFarmEnabled>
     <url type="string">/Volumes/konsistent/server/mistika-testing/prores_footage/</url>
     <autoUpdateNameConvention type="bool">false</autoUpdateNameConvention>
     <uniColorValue type="CuniColorValue">uniColor:Unknown:Unknown</uniColorValue>
     <gamma type="string">Unknown</gamma>
     <gamut type="string">Unknown</gamut>
     <codec type="string">ffmpeg_MOV.MOVIE_MOV_PRORES</codec>
     <audioCodec type="string">Null.dev</audioCodec>
     <imageResX type="int">0</imageResX>
     <imageResY type="int">0</imageResY>
     <imageFormat type="string"></imageFormat>
     <fps type="double">0</fps>
     <interlaced type="int">-1</interlaced>
     <pixelAspectRatio type="double">0</pixelAspectRatio>
     <dropFrame type="int">-1</dropFrame>
     <frames type="int">0</frames>
     <tracks type="int">0</tracks>
     <comments type="string"></comments>
     <timeLength type="string"></timeLength>
     <timeStart type="string"></timeStart>
     <timeEnd type="string"></timeEnd>
     <timeTcAux1 type="string"></timeTcAux1>
     <timeTcAux2 type="string"></timeTcAux2>
     <useMovieAudio type="bool">true</useMovieAudio>
     <resolution type="string">--- Same As Input ---</resolution>
     <interpolationType type="int">0</interpolationType>
     <trimIfOdd type="bool">false</trimIfOdd>
     <forceEvenLength type="bool">false</forceEvenLength>
     <fit type="int">1</fit>
     <scaleFilter type="int">-1</scaleFilter>
     <tapeNameSource type="int">0</tapeNameSource>
     <lut3D type="string">---None---</lut3D>
     <displayFilter type="string">---None---</displayFilter>
     <virtualSlate type="string">---None---</virtualSlate>
     <virtualSlateDuration type="int">-1</virtualSlateDuration>
     <colorSpace type="string">--- Same As Input ---</colorSpace>
     <exportCDL type="bool">false</exportCDL>
     <addTimeStamp type="bool">false</addTimeStamp>
     <changetimecode type="int">0</changetimecode>
     <timecode type="string"></timecode>
     <firstFrameFrom type="int">2</firstFrameFrom>
     <firstFrameNumber type="int">0</firstFrameNumber>
     <exportMetadata type="int">0</exportMetadata>
     <enableFrameRange type="bool">false</enableFrameRange>
     <rangeFirstFrame type="int">0</rangeFirstFrame>
     <rangeDuration type="int">0</rangeDuration>
     <packAudioTracks type="bool">true</packAudioTracks>
     <audioSampleRate type="int">0</audioSampleRate>
     <audioChannels type="int">0</audioChannels>
     <audioBitDepth type="int">0</audioBitDepth>
     <gop type="uint">50</gop>
     <bitrate type="uint">85</bitrate>
     <quality type="uint">10</quality>
    </properties>
    <connections>
     <rankFrom id="221f6629-8bd3-409a-8028-e6873ca14638" type="input" name="rankFrom" specialType="order"/>
     <rankTo id="7c2634d8-34eb-401c-a8c9-fe799835e5fd" type="output" name="rankTo" specialType="order"/>
     <VideoOut id="94a6e3f8-d7ab-4011-af52-3278a8ee46b2" type="output" name="VideoOut" label="AV Out"/>
     <AudioOut id="eb87e4b4-df65-4ed7-b270-b2f9f994457e" type="output" name="AudioOut" label="Ext. Audio" visible="0"/>
     <VideoIn id="edf578e0-3b21-4c8e-ab01-501a0096c864" type="input" name="VideoIn" label="AV In">
      <linkedTo>e0ae5b88-2d5f-4091-8f17-555e27d0956e</linkedTo>
     </VideoIn>
     <AudioIn id="5b9101fa-188e-4cf9-9a64-68675116d92f" type="input" name="AudioIn" label="Ext. Audio"/>
    </connections>
   </node>
   <node class="H264" type="task">
    <properties>
     <objectName type="string">H264Transcode</objectName>
     <color type="color">#000000</color>
     <colorByStatus type="bool">false</colorByStatus>
     <pos type="point2">-257.775,-855.01</pos>
     <schemaName type="string">h264</schemaName>
     <nameConvention type="CnameConvention"></nameConvention>
     <nodeLocked type="bool">false</nodeLocked>
     <bypassSupported type="bool">true</bypassSupported>
     <bypassEnabled type="bool">false</bypassEnabled>
     <renderFarmSupported type="bool">true</renderFarmSupported>
     <renderFarmSplitContent type="bool">false</renderFarmSplitContent>
     <farmParams type="string"></farmParams>
     <auxCode type="string"></auxCode>
     <renderFarmEnabled type="bool">false</renderFarmEnabled>
     <url type="string">/Volumes/konsistent/server/mistika-testing/input_iconik/</url>
     <autoUpdateNameConvention type="bool">false</autoUpdateNameConvention>
     <uniColorValue type="CuniColorValue">uniColor:Unknown:Unknown</uniColorValue>
     <gamma type="string">Unknown</gamma>
     <gamut type="string">Unknown</gamut>
     <codec type="string">ffmpeg_MP4.MP4_H264_BITRATE</codec>
     <audioCodec type="string">Null.dev</audioCodec>
     <imageResX type="int">0</imageResX>
     <imageResY type="int">0</imageResY>
     <imageFormat type="string"></imageFormat>
     <fps type="double">0</fps>
     <interlaced type="int">-1</interlaced>
     <pixelAspectRatio type="double">0</pixelAspectRatio>
     <dropFrame type="int">-1</dropFrame>
     <frames type="int">0</frames>
     <tracks type="int">0</tracks>
     <comments type="string"></comments>
     <timeLength type="string"></timeLength>
     <timeStart type="string"></timeStart>
     <timeEnd type="string"></timeEnd>
     <timeTcAux1 type="string"></timeTcAux1>
     <timeTcAux2 type="string"></timeTcAux2>
     <useMovieAudio type="bool">true</useMovieAudio>
     <resolution type="string">--- Same As Input ---</resolution>
     <interpolationType type="int">0</interpolationType>
     <trimIfOdd type="bool">false</trimIfOdd>
     <forceEvenLength type="bool">false</forceEvenLength>
     <fit type="int">1</fit>
     <scaleFilter type="int">-1</scaleFilter>
     <tapeNameSource type="int">0</tapeNameSource>
     <lut3D type="string">---None---</lut3D>
     <displayFilter type="string">---None---</displayFilter>
     <virtualSlate type="string">---None---</virtualSlate>
     <virtualSlateDuration type="int">-1</virtualSlateDuration>
     <colorSpace type="string">--- Same As Input ---</colorSpace>
     <exportCDL type="bool">false</exportCDL>
     <addTimeStamp type="bool">false</addTimeStamp>
     <changetimecode type="int">0</changetimecode>
     <timecode type="string"></timecode>
     <firstFrameFrom type="int">2</firstFrameFrom>
     <firstFrameNumber type="int">0</firstFrameNumber>
     <exportMetadata type="int">0</exportMetadata>
     <enableFrameRange type="bool">false</enableFrameRange>
     <rangeFirstFrame type="int">0</rangeFirstFrame>
     <rangeDuration type="int">0</rangeDuration>
     <packAudioTracks type="bool">true</packAudioTracks>
     <audioSampleRate type="int">0</audioSampleRate>
     <audioChannels type="int">0</audioChannels>
     <audioBitDepth type="int">0</audioBitDepth>
     <gop type="uint">25</gop>
     <bitrate type="uint">6</bitrate>
     <quality type="uint">10</quality>
    </properties>
    <connections>
     <rankFrom id="cba1342b-c7ec-4924-b880-a850065b1f54" type="input" name="rankFrom" specialType="order"/>
     <rankTo id="f2511470-23aa-4c60-849a-f6b042b3bcc0" type="output" name="rankTo" specialType="order"/>
     <VideoOut id="01d4d179-dcfa-4bf5-88a3-6bb7e2a6a8a5" type="output" name="VideoOut" label="AV Out">
      <linkedTo>af5aa667-ca2e-4899-a6ba-b71fc475b310</linkedTo>
     </VideoOut>
     <AudioOut id="c46f8056-a483-48c7-a43d-578f621019a5" type="output" name="AudioOut" label="Ext. Audio" visible="0"/>
     <VideoIn id="9a571c3e-f5e1-451e-b066-881408ad2b28" type="input" name="VideoIn" label="AV In">
      <linkedTo>e0ae5b88-2d5f-4091-8f17-555e27d0956e</linkedTo>
     </VideoIn>
     <AudioIn id="0871343c-64f1-4164-a8bf-61776dffc54a" type="input" name="AudioIn" label="Ext. Audio"/>
    </connections>
   </node>
   <node class="iconikIn" type="task">
    <properties>
     <objectName type="string">iconikIn</objectName>
     <color type="color">#197bbd</color>
     <colorByStatus type="bool">false</colorByStatus>
     <pos type="point2">80.5435,-1065.12</pos>
     <schemaName type="string">iconikin</schemaName>
     <nameConvention type="CnameConvention"></nameConvention>
     <nodeLocked type="bool">false</nodeLocked>
     <bypassSupported type="bool">true</bypassSupported>
     <bypassEnabled type="bool">false</bypassEnabled>
     <renderFarmSupported type="bool">false</renderFarmSupported>
     <renderFarmSplitContent type="bool">false</renderFarmSplitContent>
     <farmParams type="string"></farmParams>
     <auxCode type="string"></auxCode>
     <renderFarmEnabled type="bool">false</renderFarmEnabled>
     <code type="string"># File lives here: /Users/[username]/SGO AppData/shared/workflowsLibrary/

from Mistika.classes import Cconnector, CuniversalPath
from Mistika.Qt import QColor

import os
import re
import datetime
import mimetypes
import requests
from pathlib import Path

################################################################################
# Iconik Input Mistka Node                                                     #
# ========================                                                     #
# E.Spencer - Konsistent Consulting 2025                                       #
# Takes an input folder (watch folder/folder) and uploads it to iconik within  #
# the specified collection path. If the path does not exist, it creates the    #
# collections                                                                  #
################################################################################

# For testing:
DEFAULT_DOMAIN = "https://app.iconik.io"
DEFAULT_APP_ID = ""
DEFAULT_AUTH_TOKEN = ""

# Name of the iconik_input folder
WATCH_FOLDER = "input_iconik"


def log_info(self, tag, msg):
    """
    Safe wrapper around self.info so we don't crash on logging.
    """
    try:
        self.info(tag, msg, "")
    except Exception:
        pass


def die(self, msg):
    """
    Emit an error and return False.
    """
    try:
        self.critical("iconikIn:error", msg, "")
    except Exception:
        pass
    return False


def assert_uuid(input):
    """
    Return True if uuid is a valid UUID
    """
    string = (input or "").strip()
    return bool(
        re.fullmatch(
            r"[0-9a-fA-F]{8}-([0-9a-fA-F]{4}-){3}[0-9a-fA-F]{12}",
            string
        )
    )


def calculate_video_mime_type(path):
    """
    Calculate the MIME type based on the file extension
    """
    file_ext = path.suffix.lower()

    if file_ext == ".mp4":
        return "video/mp4"

    if file_ext == ".mov":
        return "video/quicktime"

    if file_ext == ".mxf":
        return "application/mxf"

    ctype, _ = mimetypes.guess_type(str(path))
    return ctype or "application/octet-stream"


class Iconik:
    """
    REST wrapper for iconik.
    """
    def __init__(self, domain, app_id, auth_token, tenant_id=None):
        self.domain = domain.rstrip("/")
        self.sess = requests.Session()
        self.sess.headers.update(
            {
                "App-ID": app_id,
                "Auth-Token": auth_token,
                "Accept": "application/json",
                "Content-Type": "application/json",
            }
        )
        if tenant_id:
            self.sess.headers["X-Tenant-Id"] = tenant_id

    def set_tenant(self, tenant_id):
        """
        Set or clear X-Tenant-Id.
        """
        if tenant_id:
            self.sess.headers["X-Tenant-Id"] = tenant_id
        else:
            self.sess.headers.pop("X-Tenant-Id", None)

    def _url(self, path):
        """
        Build full API URL.
        """
        if not path.startswith("/"):
            path = "/" + path
        if "?" in path:
            base, qs = path.split("?", 1)
            if not base.endswith("/"):
                base += "/"
            return f"{self.domain}{base}?{qs}"
        if not path.endswith("/"):
            path += "/"
        return f"{self.domain}{path}"


    def _check(self, r, method, path):
        """
        Raise on HTTP error.
        """
        if not r.ok:
            snip = r.text[:600]
            raise RuntimeError(f"{method} {path} -> {r.status_code}: {snip}")


    def get(self, path, params=None):
        """
        GET JSON.
        """
        r = self.sess.get(self._url(path), params=params)
        self._check(r, "GET", path)
        return r.json() if r.text else {}


    def post(self, path, payload):
        """
        POST JSON.
        """
        r = self.sess.post(self._url(path), json=payload)
        self._check(r, "POST", path)
        return r.json() if r.text else {}


    def patch(self, path, payload):
        """
        PATCH JSON.
        """
        r = self.sess.patch(self._url(path), json=payload)
        self._check(r, "PATCH", path)
        return r.json() if r.text else {}


    def search_tenant_hint(self):
        """
        Return system_domain_id from Search, or None.
        """
        body = {
            "query": "",
            "doc_types": ["collections"],
            "per_page": 1,
            "page": 1,
        }
        r = self.post("API/search/v1/search", body)
        objs = r.get("objects") or []
        return objs[0].get("system_domain_id") if objs else None


    def storage_tenant_hint(self):
        """
        Return UUID-like path from storages, or None.
        """
        r = self.get("API/files/v1/storages")
        for o in r.get("objects", []):
            p = (o.get("settings") or {}).get("path")
            if p and re.fullmatch(r"[0-9a-fA-F\-]{36}", p):
                return p
        return None


    def autodetect_tenant(self):
        """
        Try to get the iconik tenant id.
        """
        try:
            t = self.search_tenant_hint()
            if t:
                return t
        except Exception:
            pass
        try:
            t = self.storage_tenant_hint()
            if t:
                return t
        except Exception:
            pass
        return None


    def create_asset(self, title):
        """
        Create an asset within iconik
        """
        body = {
            "title": title,
            "type": "ASSET",
            "status": "ACTIVE",
            "archive_status": "NOT_ARCHIVED",
            "analyze_status": "N/A",
            "is_online": True,
        }
        return self.post("API/assets/v1/assets", body)


    def get_asset_api(self, asset_id):
        """
        Get an assets metadata.
        """
        return self.get(f"API/assets/v1/assets/{asset_id}")


    def get_matching_files_storage(self):
        """
        Get a FILES storage for upload.
        """
        return self.get("API/files/v1/storages/matching/FILES")


    def create_original_format(self, asset_id, user_id, mime):
        """
        Create ORIGINAL format.
        """
        p = {
            "user_id": user_id,
            "name": "ORIGINAL",
            "metadata": [{"internet_media_type": mime}],
            "storage_methods": ["GCS", "S3"],
        }
        return self.post(f"API/files/v1/assets/{asset_id}/formats", p)


    def create_fileset(self, asset_id, format_id, storage_id, file_name):
        """
        Create fileset
        """
        p = {
            "format_id": format_id,
            "storage_id": storage_id,
            "base_dir": "/",
            "name": file_name,
            "component_ids": [],
        }
        return self.post(f"API/files/v1/assets/{asset_id}/file_sets", p)


    def create_file(
        self,
        asset_id,
        format_id,
        fileset_id,
        storage_id,
        file_name,
        file_size,
    ):
        """
        Create file record and get upload URL.
        """
        now_iso = datetime.datetime.now().isoformat()
        p = {
            "original_name": file_name,
            "directory_path": "",
            "size": file_size,
            "type": "FILE",
            "metadata": {},
            "format_id": format_id,
            "file_set_id": fileset_id,
            "storage_id": storage_id,
            "file_date_created": now_iso,
            "file_date_modified": now_iso,
        }
        return self.post(f"API/files/v1/assets/{asset_id}/files", p)


    def compose_gcs(self, asset_id, file_id, content_type):
        """
        Compose GCS multipart upload.
        """
        p = {"parts_group": None, "content_type": content_type}
        return self.post(
            f"API/files/v1/assets/{asset_id}/files/{file_id}/multipart/"
            "gcs/compose_url",
            p,
        )


    def close_file(self, asset_id, file_id):
        """
        Close file after upload.
        """
        return self.patch(
            f"API/files/v1/assets/{asset_id}/files/{file_id}",
            {"status": "CLOSED", "progress_processed": 100},
        )


    def generate_keyframes(self, asset_id, file_id):
        """
        Request keyframes (best-effort)
        """
        return self.post(
            f"API/files/v1/assets/{asset_id}/files/{file_id}/keyframes",
            {}
        )


    def start_job(self, asset_id, title):
        """
        Start TRANSFER job.
        """
        p = {
            "object_type": "assets",
            "object_id": asset_id,
            "type": "TRANSFER",
            "status": "STARTED",
            "title": title,
        }
        return self.post("API/jobs/v1/jobs", p)


    def finish_job(self, job_id):
        """
        Finish job and set 100% progress.
        """
        return self.patch(
            f"API/jobs/v1/jobs/{job_id}",
            {"progress_processed": 100, "status": "FINISHED"},
        )


    def add_asset_to_collection(self, collection_id, asset_id):
        """
        Link asset into a collection.
        """
        p = {"object_type": "assets", "object_id": asset_id}
        return self.post(
            f"API/assets/v1/collections/{collection_id}/contents",
            p,
        )


    def list_collection_contents(self, collection_id, page=1, per_page=200):
        """
        List collection contents
        """
        return self.get(
            f"API/assets/v1/collections/{collection_id}/contents",
            params={"page": page, "per_page": per_page},
        )


    def list_collections_api(self, page=1, per_page=200,
                             include_deleted=False, include_smart=False):
        """
        List collections via ASSETS API.
        """
        params = {"page": page, "per_page": per_page}
        data = self.get("API/assets/v1/collections", params)
        out = []
        for o in data.get("objects", []) or []:
            status = (o.get("status") or "").upper()
            ctype = (o.get("type") or "").upper()
            if not include_deleted and status == "DELETED":
                continue
            if not include_smart and ctype == "SMART":
                continue
            out.append(o)
        pages = data.get("pages") or 1
        return out, pages


    def get_collection_api(self, collection_id):
        """
        Get collection metadata
        """
        return self.get(f"API/assets/v1/collections/{collection_id}")


    def create_collection_api(self, title, parent_id=None):
        """
        Create a static collection with optional parent
        """
        body = {"title": title, "status": "ACTIVE"}
        if parent_id:
            body["parent_id"] = parent_id
        return self.post("API/assets/v1/collections", body)


def s3_direct_put(upload_url, file_path, maybe_headers=None):
    """
    Upload a file to S3 via signed PUT.
    """
    headers = {}
    if maybe_headers:
        headers.update(maybe_headers)

    # Normalise header case
    lower = {k.lower(): k for k in headers.keys()}
    if "content-type" not in lower:
        ctype, _ = mimetypes.guess_type(str(file_path))
        if ctype:
            headers["Content-Type"] = ctype
    headers["Content-Length"] = str(file_path.stat().st_size)
    with file_path.open("rb") as f:
        r = requests.put(upload_url, data=f, headers=headers)
    if r.status_code not in (200, 201, 204):
        raise RuntimeError(f"S3 PUT -> {r.status_code}: {r.text[:600]}")


def gcs_resumable_upload(upload_url, file_path, file_size, origin):
    """
    Upload a file to GCS via resumable API.
    """
    start_headers = {
        "accept": "application/json, text/plain, */*",
        "origin": origin,
        "referer": f"{origin}/upload",
        "x-goog-resumable": "start",
    }
    s = requests.post(upload_url, headers=start_headers)
    if s.status_code not in (200, 201):
        raise RuntimeError(f"GCS start -> {s.status_code}: {s.text[:600]}")
    upload_id = s.headers.get("X-GUploader-UploadID")
    if not upload_id:
        raise RuntimeError("No X-GUploader-UploadID from GCS start.")
    put_headers = {
        "content-length": str(file_size),
        "content-type": "application/octet-stream",
        "origin": origin,
        "referer": f"{origin}/upload",
        "x-goog-resumable": "start",
    }
    full_url = f"{upload_url}&amp;upload_id={upload_id}"
    with file_path.open("rb") as f:
        p = requests.put(full_url, headers=put_headers, data=f)
    if p.status_code not in (200, 201):
        raise RuntimeError(f"GCS PUT -> {p.status_code}: {p.text[:600]}")


def _child_collection_by_title(ik, parent_id, title):
    """
    Return child collection id with exact title, or None.
    """
    page, per_page = 1, 200
    while True:
        res = ik.list_collection_contents(parent_id, page=page,
                                          per_page=per_page)
        objs = res.get("objects") or []
        for o in objs:
            ot = (o.get("object_type") or o.get("type") or "").lower()
            if ot != "collections":
                continue
            cid = o.get("object_id") or o.get("id")
            if not cid:
                continue
            meta = ik.get_collection_api(cid)
            t = meta.get("title") or meta.get("name") or ""
            if t == title:
                return cid
        pages = res.get("pages") or 1
        if page >= pages:
            break
        page += 1
    return None


def _root_candidates_by_title(ik, title):
    """
    Return ids of collections with this top-level title
    """
    ids = []
    page = 1
    while True:
        objs, pages = ik.list_collections_api(
            page=page,
            per_page=200,
            include_deleted=False,
            include_smart=False,
        )
        for o in objs:
            t = o.get("title") or o.get("name") or ""
            if t == title and o.get("id"):
                ids.append(o["id"])
        if page >= pages:
            break
        page += 1
    return ids


def ensure_collection_path(ik, path_str):
    """
    Ensure 'A/B/C' exists as a collection hierarchy.
    Returns the final collection id, creating any missing parts.
    """
    parts = [p.strip() for p in (path_str or "").split("/") if p.strip()]
    if not parts:
        return None

    roots = _root_candidates_by_title(ik, parts[0])

    if roots:
        root_id = roots[0]
    else:
        created_root = ik.create_collection_api(parts[0])
        root_id = created_root.get("id")

    if len(parts) > 1:
        return ensure_subpath_collections(ik, root_id, parts[1:])

    return root_id


def resolve_collection_path(ik, path_str):
    """
    Resolve 'A/B/C' to an existing collection id (no creation)
    """
    parts = [p.strip() for p in (path_str or "").split("/")
             if p.strip()]
    if not parts:
        return None

    roots = _root_candidates_by_title(ik, parts[0])

    def walk_from(root_id):
        current = root_id
        for name in parts[1:]:
            child = _child_collection_by_title(ik, current, name)
            if not child:
                return None
            current = child
        return current

    for rid in roots:
        final = walk_from(rid)
        if final:
            return final
    return None


def ensure_subpath_collections(ik, base_coll_id, parts):
    """
    Under base_coll_id, ensure sub-collections for parts[0]/.../parts[-1]
    exist. Return the final collection id.
    """
    current = base_coll_id
    for name in parts:
        if not name:
            continue
        child = _child_collection_by_title(ik, current, name)
        if child:
            current = child
            continue
        created = ik.create_collection_api(name, parent_id=current)
        current = created.get("id") or current
    return current


def _build_collection_asset_title_cache(ik, collection_id):
    """
    Build a set of existing asset titles (lowercased) for a collection.
    """
    titles = set()
    page, per_page = 1, 200
    while True:
        res = ik.list_collection_contents(collection_id, page=page,
                                          per_page=per_page)
        objs = res.get("objects") or []
        for o in objs:
            ot = (o.get("object_type") or o.get("type") or "").lower()
            if ot != "assets":
                continue
            aid = o.get("object_id") or o.get("id")
            if not aid:
                continue
            try:
                meta = ik.get_asset_api(aid)
            except Exception:
                continue
            t = (meta.get("title") or meta.get("name") or "").strip()
            if t:
                titles.add(t.lower())
        pages = res.get("pages") or 1
        if page >= pages:
            break
        page += 1
    return titles


def asset_exists_in_collection_by_title(ik, collection_id, title,
                                        cache):
    """
    Return True if collection already has an asset with given title.
    'cache' is a dict[collection_id] -> set(lowercased titles).
    """
    if not collection_id or not title:
        return False
    key = collection_id
    if key not in cache:
        cache[key] = _build_collection_asset_title_cache(ik, key)
    return title.lower() in cache[key]


def _parse_extensions(ext_string):
    """
    Parse '.mov,.mp4;.mxf' into a set of lower-case extensions
    """
    raw = (ext_string or "").strip().lower()
    if not raw:
        return set()
    parts = re.split(r"[,\s;]+", raw)
    exts = set()
    for p in parts:
        if not p:
            continue
        if not p.startswith("."):
            p = "." + p
        exts.add(p)
    return exts


def _get_input_paths_from_connector(self):
    """
    Return a list of Path objects from the 'in' connector.
    If the connector path is a directory (e.g. watcher content root),
    recursively collect files below that directory matching fileTypes.
    """
    in_conn = self.getFirstConnectorByName("in")
    if in_conn is None:
        log_info(self, "iconikIn:connector", "No 'in' connector found")
        return []

    ups = in_conn.getUniversalPaths()
    if not ups:
        log_info(self, "iconikIn:connector", "'in' connector has no UPs")
        return []

    exts = _parse_extensions(getattr(self, "fileTypes", ""))
    log_info(self, "iconikIn:connector",
             f"'in' connector has {len(ups)} UP(s); extensions={sorted(exts) or 'ALL'}")

    paths = []
    for up in ups:
        try:
            up_path = up.getPath()
        except Exception:
            up_path = ""
        log_info(self, "iconikIn:connectorUP", f"UP path='{up_path}'")

        try:
            p = Path(up_path).expanduser().resolve()
        except Exception:
            continue

        if not p.exists():
            log_info(self, "iconikIn:connectorUP",
                     f"Path does not exist, skipping: {p}")
            continue

        if p.is_file():
            if exts and p.suffix.lower() not in exts:
                continue
            paths.append(p)
        elif p.is_dir():
            for child in sorted(p.rglob("*")):
                if not child.is_file():
                    continue
                if exts and child.suffix.lower() not in exts:
                    continue
                paths.append(child.resolve())

    log_info(self, "iconikIn:connector",
             f"Total files gathered from connector: {len(paths)}")
    return paths


def _find_watch_root(path_str):
    """
    Given a filesystem path to a *directory* (usually file_path.parent),
    find the directory that should be treated as the logical WATCH_FOLDER.

    WATCH_FOLDER can be:
      - A folder name, e.g. "input_iconik"
      - An absolute path, e.g. "/Volumes/.../input_iconik"

    Returns the path string of that root, or None if not found.
    """
    cfg = (WATCH_FOLDER or "").strip()
    if not cfg:
        return None

    norm_path = os.path.normpath(path_str)

    # If WATCH_FOLDER is an absolute path, use it directly if norm_path is under it
    if os.path.isabs(cfg):
        root_abs = os.path.normpath(cfg)
        try:
            common = os.path.commonpath([norm_path, root_abs])
            if common == root_abs:
                return root_abs
        except Exception:
            pass

    # Otherwise, treat WATCH_FOLDER as a folder name and walk up the ancestors
    wanted_name = os.path.basename(cfg)
    try:
        cur = norm_path
        while True:
            if os.path.basename(cur) == wanted_name:
                return cur
            parent = os.path.dirname(cur)
            if parent == cur:
                break
            cur = parent
    except Exception:
        pass

    return None


def _relative_subfolder_parts(file_path, folder_root):
    """
    Compute subfolder parts for mirroring, based on:

      - Preferred: the configured WATCH_FOLDER (no part of the path above
        WATCH_FOLDER is ever turned into a collection), e.g.:

          /Volumes/.../input_iconik/file.mov
            -> []
          /Volumes/.../input_iconik/folder/sub/file.mov
            -> ["folder", "sub"]

      - Fallback: folder_root (derived from common parent of all input files)
      - Last resort: just [parent.name]
    """
    parent = file_path.parent
    norm_parent = os.path.normpath(str(parent))

    watch_root = _find_watch_root(norm_parent)
    if watch_root:
        try:
            rel = os.path.relpath(norm_parent, start=watch_root)
            if rel in (".", ""):
                return []
            parts = [p for p in rel.split(os.sep) if p and p not in (".", "/")]
            return parts
        except Exception:
            pass

    if folder_root:
        try:
            norm_root = os.path.normpath(str(folder_root))
            rel = os.path.relpath(norm_parent, start=norm_root)
            if rel in (".", ""):
                return []
            parts = [p for p in rel.split(os.sep) if p and p not in (".", "/")]
            return parts
        except Exception:
            pass

    return [parent.name] if parent.name else []


def _expand_tokens_from_up(template, context_up):
    """
    Replace [token] placeholders in 'template' with values taken from
    the UniversalPath's param bag (context_up.getParam(token, "")).

    - Works for ANY token name present on the UP.
    - If a token has no value, the original [token] text is left intact.
    """
    if not template or context_up is None:
        return template

    def repl(match):
        name = (match.group(1) or "").strip()
        if not name:
            return match.group(0)
        try:
            val = context_up.getParam(name, "")
        except Exception:
            val = ""
        return val if val not in (None, "") else match.group(0)

    return re.sub(r"\[([^\]]+)\]", repl, template)


def init(self):
    """
    Define connectors and GUI properties.
    """
    self.setClassName("iconikIn")
    self.color = QColor(0x19, 0x7B, 0xBD)

    # Input from Watcher / other upstream + output to the rest.
    self.addConnector(
        "in", Cconnector.CONNECTOR_TYPE_INPUT, Cconnector.MODE_OPTIONAL
    )
    self.addConnector(
        "out", Cconnector.CONNECTOR_TYPE_OUTPUT, Cconnector.MODE_OPTIONAL
    )

    # Accept connections on 'in'
    self.setAcceptConnectors(True, "in")

    # Credentials
    self.addProperty("iconik_appId", DEFAULT_APP_ID)
    self.addEncryptedProperty("iconik_authToken", DEFAULT_AUTH_TOKEN)
    self.addProperty("iconik_domain", DEFAULT_DOMAIN)

    # Optional local input folder
    self.addProperty("fileTypes", ".mov,.mp4,.mxf")

    # Collection targeting
    self.addProperty("collectionPath", "[brand]/[campaign]/[job]")

    # Mirror folder structure into sub-collections
    self.addProperty("mirrorSubfolders", True)

    # Asset options
    self.addProperty("skipKeyframes", False)

    self.addProperty("objectName", "")

    self.bypassSupported = True

    # Treat as a TASK node so it can sit mid-pipeline and pass UPs through
    self.setSupportedTypes(self.NODETYPE_TASK)
    self.setComplexity(150)
    return True


def isReady(self):
    """
    Check node is ready to run (config only)
    """
    if self.bypassSupported and self.bypassEnabled:
        return True

    ok = True
    app_id = (self.iconik_appId or "").strip()
    auth = (self.iconik_authToken or "").strip()
    dom = (self.iconik_domain or "").strip()

    collection_path = (self.collectionPath or "").strip()

    if app_id == "":
        ok = self.critical("iconikIn:iconik_appId", "'iconik_appId' required", "") and ok
    if auth == "":
        ok = self.critical("iconikIn:iconik_authToken", "'iconik_authToken' required", "") and ok
    if dom == "":
        ok = self.critical("iconikIn:iconik_domain", "'iconik_domain' required", "") and ok
    if collection_path == "":
        ok = self.critical(
            "iconikIn:collectionPath",
            "collectionPath is required",
            ""
        ) and ok

    return ok


def process(self):
    """
    Upload each incoming file and link to collection / sub-collections.
    Also: pass-through all input UPs unchanged on 'out' (if present).
    """
    log_info(self, "iconikIn:process", "Process started")

    out = self.getFirstConnectorByName("out")
    if out:
        out.clearUniversalPaths()

    # Gather input UPs for later pass-through
    inputs = self.getConnectorsByType(Cconnector.CONNECTOR_TYPE_INPUT)
    ups_in = []
    for c in inputs:
        ups_in.extend(c.getUniversalPaths() or [])

    log_info(self, "iconikIn:ups",
             f"Collected {len(ups_in)} incoming UP(s)")

    # BYPASS: pass-through input file list unchanged
    if self.bypassSupported and self.bypassEnabled:
        log_info(self, "iconikIn:process",
                 "Bypass enabled – passing input UPs unchanged")
        if out and ups_in:
            for up in ups_in:
                out.addUniversalPath(up)
            log_info(
                self,
                "iconikIn:process",
                f"Bypass: {len(ups_in)} UP(s) sent to 'out'"
            )
        return True

    app_id = (self.iconik_appId or "").strip()
    auth = (self.iconik_authToken or "").strip()
    dom = (self.iconik_domain or DEFAULT_DOMAIN).strip()
    base_coll_id = None

    log_info(
        self,
        "iconikIn:config",
        f"domain='{dom}', appId set={bool(app_id)}, authToken set={bool(auth)}"
    )

    log_info(
        self,
        "iconikIn:config",
        f"collectionPath='{self.collectionPath}'"
    )

    try:
        raw_coll_path = self.evaluate(self.collectionPath).strip()
    except Exception:
        raw_coll_path = (self.collectionPath or "").strip()

    log_info(
        self,
        "iconikIn:collectionPath",
        f"Raw (after evaluate) collectionPath='{raw_coll_path}'"
    )

    in_conn = self.getFirstConnectorByName("in")
    context_up = None
    if in_conn:
        ups = in_conn.getUniversalPaths() or []
        if ups:
            context_up = ups[0]
            try:
                ctx_path = context_up.getPath()
            except Exception:
                ctx_path = ""
            log_info(
                self,
                "iconikIn:contextUP",
                f"Using first UP as token context: path='{ctx_path}'"
            )
        else:
            log_info(self, "iconikIn:contextUP", "No UPs on 'in' connector")
    else:
        log_info(self, "iconikIn:contextUP", "No 'in' connector")

    coll_path = _expand_tokens_from_up(raw_coll_path, context_up)

    log_info(
        self,
        "iconikIn:collectionPath",
        f"Template='{raw_coll_path}'  Expanded='{coll_path}'"
    )

    if not coll_path:
        return die(self, "collectionPath evaluated to an empty string")

    mirror = bool(self.mirrorSubfolders)
    log_info(self, "iconikIn:mirror", f"mirrorSubfolders={mirror}")

    in_paths = _get_input_paths_from_connector(self)
    folder_root = None

    try:
        common = os.path.commonpath([str(p.parent) for p in in_paths])
        folder_root = Path(common)
        log_info(
            self,
            "iconikIn:paths",
            f"Derived folder_root from connector files: {folder_root}"
        )
    except Exception as e:
        log_info(
            self,
            "iconikIn:paths",
            f"Failed to derive folder_root: {e}"
        )
        folder_root = None

    if not in_paths:
        return die(self, "No files connected")
    else:
        log_info(
            self,
            "iconikIn:paths",
            f"Total input files to process: {len(in_paths)}"
        )

    try:
        log_info(self, "iconikIn:iconik", "Creating Iconik client")
        ik = Iconik(dom, app_id, auth, tenant_id=None)
        t = ik.autodetect_tenant()
        if t:
            ik.set_tenant(t)
            log_info(
                self,
                "iconikIn:iconik",
                f"Tenant autodetected and set: {t}"
            )
        else:
            log_info(
                self,
                "iconikIn:iconik",
                "Tenant autodetection failed or not needed"
            )

        if coll_path:
            log_info(
                self,
                "iconikIn:collectionResolve",
                f"Resolving collectionPath in iconik: '{coll_path}'"
            )
            cid = resolve_collection_path(ik, coll_path)
            if cid:
                log_info(
                    self,
                    "iconikIn:collectionResolve",
                    f"Existing collectionPath found, id={cid}"
                )
            if not cid:
                log_info(
                    self,
                    "iconikIn:collectionResolve",
                    "collectionPath not found, creating hierarchy"
                )
                cid = ensure_collection_path(ik, coll_path)
                if not cid:
                    return die(
                        self,
                        "Failed to create collectionPath hierarchy",
                    )
                log_info(
                    self,
                    "iconikIn:collectionResolve",
                    f"Created collectionPath hierarchy, id={cid}"
                )
            base_coll_id = cid

        log_info(
            self,
            "iconikIn:collection",
            f"Base collection id to use: '{base_coll_id}'"
        )

        collection_titles_cache = {}
        uploaded_count = 0

        for video in in_paths:
            log_info(self, "iconikIn:file", f"Processing file: {video}")

            if not video.exists() or not video.is_file():
                log_info(
                    self,
                    "iconikIn:file",
                    f"Skipping – not a valid file: {video}"
                )
                continue

            title = video.stem
            skip_kf = bool(self.skipKeyframes)

            log_info(
                self,
                "iconikIn:file",
                f"Asset title='{title}', skipKeyframes={skip_kf}"
            )

            target_coll = base_coll_id
            if mirror and base_coll_id:
                parts = _relative_subfolder_parts(video, folder_root)
                log_info(
                    self,
                    "iconikIn:subcollections",
                    f"Subfolder parts for mirror: {parts}"
                )
                if parts:
                    target_coll = ensure_subpath_collections(
                        ik, base_coll_id, parts
                    )
                    log_info(
                        self,
                        "iconikIn:subcollections",
                        f"Ensured subpath collections -> target_coll={target_coll}"
                    )

            if target_coll and asset_exists_in_collection_by_title(
                ik, target_coll, title, collection_titles_cache
            ):
                log_info(
                    self,
                    "iconikIn:dedupe",
                    f"Asset titled '{title}' already exists in collection "
                    f"{target_coll}, skipping upload"
                )
                continue

            mime = calculate_video_mime_type(video)
            log_info(
                self,
                "iconikIn:file",
                f"MIME type for '{video.name}' is '{mime}'"
            )

            asset = ik.create_asset(title)
            asset_id = asset["id"]
            user_id = asset["created_by_user"]
            log_info(
                self,
                "iconikIn:file",
                f"Created asset id={asset_id}, user_id={user_id}"
            )

            store = ik.get_matching_files_storage()
            storage_id = store["id"]
            method = (store.get("method") or "").upper()
            log_info(
                self,
                "iconikIn:file",
                f"Using storage id={storage_id}, method={method}"
            )

            fmt = ik.create_original_format(asset_id, user_id, mime)
            format_id = fmt["id"]
            log_info(
                self,
                "iconikIn:file",
                f"Created ORIGINAL format id={format_id}"
            )

            fset = ik.create_fileset(
                asset_id, format_id, storage_id, video.name
            )
            fileset_id = fset["id"]
            log_info(
                self,
                "iconikIn:file",
                f"Created fileset id={fileset_id} name={video.name}"
            )

            finfo = ik.create_file(
                asset_id,
                format_id,
                fileset_id,
                storage_id,
                video.name,
                video.stat().st_size,
            )
            upload_url = finfo.get("upload_url")
            file_id = finfo.get("id")
            up_headers = (
                finfo.get("headers") or finfo.get("upload_headers") or {}
            )
            log_info(
                self,
                "iconikIn:file",
                f"Created file id={file_id}, "
                f"upload_url present={bool(upload_url)}"
            )

            if not upload_url or not file_id:
                return die(
                    self,
                    "unexpected create_file response "
                    "(no upload_url or file_id)"
                )

            job = ik.start_job(asset_id, f"Upload {video.name}")
            job_id = job["id"]
            log_info(
                self,
                "iconikIn:file",
                f"Started TRANSFER job id={job_id} for asset_id={asset_id}"
            )

            if method == "S3" or (upload_url and "amazonaws.com" in upload_url.lower()):
                log_info(
                    self,
                    "iconikIn:upload",
                    f"Performing S3 direct PUT to {upload_url}"
                )
                s3_direct_put(upload_url, video, up_headers)
            else:
                log_info(
                    self,
                    "iconikIn:upload",
                    "Performing GCS resumable upload"
                )
                gcs_resumable_upload(
                    upload_url,
                    video,
                    video.stat().st_size,
                    dom,
                )
                ik.compose_gcs(
                    asset_id, file_id, "application/octet-stream"
                )

            ik.close_file(asset_id, file_id)
            log_info(self, "iconikIn:file", f"Closed file id={file_id}")

            if not skip_kf:
                try:
                    ik.generate_keyframes(asset_id, file_id)
                    log_info(
                        self,
                        "iconikIn:keyframes",
                        f"Keyframes requested for asset_id={asset_id}, "
                        f"file_id={file_id}"
                    )
                except Exception as e:
                    log_info(
                        self,
                        "iconikIn:keyframes",
                        f"Keyframe generation failed (non-fatal): {e}"
                    )

            if target_coll:
                ik.add_asset_to_collection(target_coll, asset_id)
                log_info(
                    self,
                    "iconikIn:collection",
                    f"Added asset {asset_id} to collection {target_coll}"
                )
                collection_titles_cache.setdefault(
                    target_coll, set()
                ).add(title.lower())

            ik.finish_job(job_id)
            log_info(self, "iconikIn:file", f"Finished job id={job_id}")

            uploaded_count += 1

        if uploaded_count == 0:
            return die(self, "no valid files were uploaded")

        # OUTPUT: pass-through original UPs if present; otherwise synthesize UPs from files
        if out:
            if ups_in:
                for up in ups_in:
                    out.addUniversalPath(up)
                log_info(
                    self,
                    "iconikIn:process",
                    f"Process finished, {len(ups_in)} UP(s) sent to 'out' (pass-through)"
                )
            else:
                for video in in_paths:
                    up = CuniversalPath()
                    up.setPath(str(video))
                    out.addUniversalPath(up)
                log_info(
                    self,
                    "iconikIn:process",
                    f"Process finished, {len(in_paths)} UP(s) sent to 'out' from in_paths"
                )

        return True

    except Exception as e:
        log_info(self, "iconikIn:exception", f"Exception in process(): {e}")
        return die(self, "iconik upload failed: %s" % str(e))


def onPropertyUpdated(self, name):
    """
    Property-change hook (currently unused).
    """
    try:
        pass
    except AttributeError:
        pass
</code>
     <iconik_appId type="string">8518e744-c53d-11f0-a825-063ff9288a04</iconik_appId>
     <iconik_authToken encrypted="1" type="string">AwWYmqzY0zXRAlaO1nL99mcSPkgQmagRHj08bvlITz8+n29RXCnMpwBLmCVgZqNjOIJIjmerDd0DwADeeAkuylkB2A8ZCCEaHyOxmCbS2HKn7Ones8cxerlrfIUDT7rJuQpb0/D959vqqdG7ZX+FzfbvnNNWwBeKIgR+vbBNrh2hU9YAjl4iZwsArQxnGFPMoyEyEzC9dD8e8x/J0C6RKdXaJMDYYVRr+G5B0Q9dYp8qo+pfgYX23O+2excSxBYhip8n0fuk0PKAohLJxT0=</iconik_authToken>
     <iconik_domain type="string">https://app.iconik.io</iconik_domain>
     <fileTypes type="string">.mov,.mp4,.mxf</fileTypes>
     <collectionPath type="string">[brand]/[campaign]/[job]</collectionPath>
     <mirrorSubfolders type="bool">true</mirrorSubfolders>
     <skipKeyframes type="bool">false</skipKeyframes>
    </properties>
    <connections>
     <rankFrom id="7d4e0ff7-9490-4d96-90fd-616ca9f18c16" type="input" name="rankFrom" specialType="order"/>
     <rankTo id="04f3bec8-ba4f-4033-bd84-8e6438a26152" type="output" name="rankTo" specialType="order">
      <linkedTo>5a09f0ab-1b68-4e0f-b378-6fdbf966404d</linkedTo>
     </rankTo>
     <in id="af5aa667-ca2e-4899-a6ba-b71fc475b310" type="input" name="in">
      <linkedTo>01d4d179-dcfa-4bf5-88a3-6bb7e2a6a8a5</linkedTo>
     </in>
     <out id="b97e1c79-2ac8-4204-b01a-db7b164d385f" type="output" name="out"/>
    </connections>
   </node>
   <node class="Folder" type="input">
    <properties>
     <objectName type="string">input_iconik</objectName>
     <color type="color">#808000</color>
     <colorByStatus type="bool">false</colorByStatus>
     <pos type="point2">70.2752,-862.278</pos>
     <schemaName type="string">folder</schemaName>
     <nameConvention type="CnameConvention">[path][baseName][.frame][.ext]</nameConvention>
     <nodeLocked type="bool">false</nodeLocked>
     <bypassSupported type="bool">true</bypassSupported>
     <bypassEnabled type="bool">false</bypassEnabled>
     <renderFarmSupported type="bool">false</renderFarmSupported>
     <renderFarmSplitContent type="bool">false</renderFarmSplitContent>
     <farmParams type="string"></farmParams>
     <auxCode type="string"></auxCode>
     <renderFarmEnabled type="bool">false</renderFarmEnabled>
     <path type="string">/Volumes/konsistent/server/mistika-testing/input_iconik/</path>
     <filterMode type="int">1</filterMode>
     <fileNameOnly type="bool">true</fileNameOnly>
     <include type="string"></include>
     <exclude type="string"></exclude>
     <addRoot type="bool">true</addRoot>
     <recursive type="bool">true</recursive>
     <maintainPadding type="bool">true</maintainPadding>
     <alphabeticalOrder type="bool">true</alphabeticalOrder>
     <includeHidden type="bool">false</includeHidden>
     <includeSystem type="bool">false</includeSystem>
    </properties>
    <connections>
     <rankFrom id="5a09f0ab-1b68-4e0f-b378-6fdbf966404d" type="input" name="rankFrom" specialType="order">
      <linkedTo>04f3bec8-ba4f-4033-bd84-8e6438a26152</linkedTo>
     </rankFrom>
     <rankTo id="56fa5e96-d35f-4de5-8263-cb4766d321b5" type="output" name="rankTo" specialType="order"/>
     <Files id="01f01856-47bc-482e-8e4a-8bf6d5367e2d" type="output" name="Files">
      <linkedTo>e2a14554-6479-4210-890f-7b4c01196d50</linkedTo>
     </Files>
    </connections>
   </node>
   <node class="Move" type="task">
    <properties>
     <objectName type="string">MoveIconikfiles</objectName>
     <color type="color">#677688</color>
     <colorByStatus type="bool">false</colorByStatus>
     <pos type="point2">392.931,-857.943</pos>
     <schemaName type="string">move</schemaName>
     <nameConvention type="CnameConvention"></nameConvention>
     <nodeLocked type="bool">false</nodeLocked>
     <bypassSupported type="bool">true</bypassSupported>
     <bypassEnabled type="bool">false</bypassEnabled>
     <renderFarmSupported type="bool">false</renderFarmSupported>
     <renderFarmSplitContent type="bool">false</renderFarmSplitContent>
     <farmParams type="string"></farmParams>
     <auxCode type="string"></auxCode>
     <renderFarmEnabled type="bool">false</renderFarmEnabled>
     <url type="string">/Volumes/konsistent/server/mistika-testing/_done/old_iconik_uploads/</url>
     <autoUpdateNameConvention type="bool">false</autoUpdateNameConvention>
     <overwrite type="int">1</overwrite>
    </properties>
    <connections>
     <rankFrom id="45985763-e310-4b6f-98a9-2804abbd1d45" type="input" name="rankFrom" specialType="order"/>
     <rankTo id="f82815a1-d3be-4cc5-9998-22b1e5004bdf" type="output" name="rankTo" specialType="order"/>
     <File id="e2a14554-6479-4210-890f-7b4c01196d50" type="input" name="File">
      <linkedTo>01f01856-47bc-482e-8e4a-8bf6d5367e2d</linkedTo>
     </File>
     <Files id="0598ad33-ae27-4d85-8d6f-217eb41de26a" type="output" name="Files"/>
    </connections>
   </node>
   <node class="S3OutSubFolders" type="output">
    <properties>
     <objectName type="string">S3OutSubFolders</objectName>
     <color type="color">#e19900</color>
     <colorByStatus type="bool">false</colorByStatus>
     <pos type="point2">-234.758,-430.388</pos>
     <schemaName type="string">s3outsubfolders</schemaName>
     <nameConvention type="CnameConvention"></nameConvention>
     <nodeLocked type="bool">false</nodeLocked>
     <bypassSupported type="bool">true</bypassSupported>
     <bypassEnabled type="bool">false</bypassEnabled>
     <renderFarmSupported type="bool">false</renderFarmSupported>
     <renderFarmSplitContent type="bool">false</renderFarmSplitContent>
     <farmParams type="string"></farmParams>
     <auxCode type="string"></auxCode>
     <renderFarmEnabled type="bool">false</renderFarmEnabled>
     <code type="string">from Mistika.classes import Cconnector
from Mistika.Qt import QColor
from wfAWS import wfAWS
import os

################################################################################
# Iconik S3 Output with Sub Folders                                            #
# =================================                                            #
# E.Spencer - Konsistent Consulting 2025                                       #
# Uploads given path to S3 and preserves sub folders as it uploads. It also    #
# uses dynamic tokens when generating the s3 path.                             #
################################################################################

INPUT_ROOT = "INPUT"

def _expand_tokens_from_up(template, context_up):
    """
    Replace [token] placeholders in 'template' with values taken from
    the UniversalPath's param bag (context_up.getParam(token, "")).
    If a token has no value, the original [token] text is left intact.
    """
    import re

    if not template or context_up is None:
        return template

    def repl(match):
        name = (match.group(1) or "").strip()
        if not name:
            return match.group(0)
        try:
            val = context_up.getParam(name, "")
        except Exception:
            val = ""
        return val if val not in (None, "") else match.group(0)

    return re.sub(r"\[([^\]]+)\]", repl, template)


def _find_input_root(path_str):
    """
    Given a filesystem path, find the directory that should be treated
    as the logical 'INPUT' root.

    INPUT_ROOT can be:
      - A folder name, e.g. "INPUT"
      - An absolute path, e.g. "/Volumes/.../INPUT"

    Returns the path string of that root, or None if not found.
    """
    cfg = (INPUT_ROOT or "").strip()
    if not cfg:
        return None

    norm_path = os.path.normpath(path_str)

    if os.path.isabs(cfg):
        root_abs = os.path.normpath(cfg)
        try:
            common = os.path.commonpath([norm_path, root_abs])
            if common == root_abs:
                return root_abs
        except Exception:
            pass

    wanted_name = os.path.basename(cfg)
    try:
        cur = norm_path
        while True:
            if os.path.basename(cur) == wanted_name:
                return cur
            parent = os.path.dirname(cur)
            if parent == cur:
                break
            cur = parent
    except Exception:
        pass

    return None


def init(self):
    self.setClassName("S3OutSubFolders")
    self.color = QColor(0xe19900)
    self.addConnector("in", Cconnector.CONNECTOR_TYPE_INPUT, Cconnector.MODE_OPTIONAL)
    self.addConnector("out", Cconnector.CONNECTOR_TYPE_OUTPUT, Cconnector.MODE_OPTIONAL)
    self.addProperty("aws_bucket")
    self.addProperty("aws_key")
    self.addEncryptedProperty("aws_secret")
    self.addProperty("objectName")
    self.bypassSupported = True
    self.setSupportedTypes(self.NODETYPE_OUTPUT)
    self.setComplexity(100)
    self.setAcceptConnectors(True, "in")
    return True


def isReady(self):
    if self.bypassSupported and self.bypassEnabled:
        return True
    res = True
    try:
        bucket = self.evaluate(self.aws_bucket).strip()
    except Exception:
        bucket = (self.aws_bucket or "").strip()
    key = (self.aws_key or "").strip()
    secret = (self.aws_secret or "").strip()
    if key == "":
        res = self.critical("s3Out:aws_key", "'aws_key' can not be empty") and res
    if secret == "":
        res = self.critical("s3Out:aws_secret", "'aws_secret' can not be empty") and res
    if bucket == "":
        res = self.critical("s3Out:aws_bucket", "'aws_bucket' can not be empty") and res
    return res


def process(self):
    inputs = self.getConnectorsByType(Cconnector.CONNECTOR_TYPE_INPUT)
    out = self.getFirstConnectorByName("out")
    if out:
        out.clearUniversalPaths()

    if self.bypassEnabled:
        for c in inputs:
            for up in c.getUniversalPaths():
                out.addUniversalPath(up)
        return True

    res = True

    ups_in = []
    for c in inputs:
        ups_in.extend(c.getUniversalPaths() or [])

    context_up = ups_in[0] if ups_in else None

    try:
        raw_bucket = self.evaluate(self.aws_bucket).strip()
    except Exception:
        raw_bucket = (self.aws_bucket or "").strip()

    expanded_bucket = _expand_tokens_from_up(raw_bucket, context_up)

    partes = (expanded_bucket or "").split('/', 1)
    bucket = partes[0]
    folder_template = partes[1] if len(partes) > 1 else ""

    key = (self.aws_key or "").strip()
    secret = (self.aws_secret or "").strip()
    aws = wfAWS(self)

    if not aws.connect(key, secret) or not aws.checkIfBucketExists(bucket):
        return False

    def _get_up_root_dir(up):
        for attr in ("getBasePath", "getRootPath", "getPath", "getFolder", "getDirname"):
            getter = getattr(up, attr, None)
            if callable(getter):
                try:
                    p = getter()
                    if isinstance(p, str) and os.path.isdir(p):
                        return os.path.normpath(p)
                except Exception:
                    pass

        try:
            files = up.getAllFiles()
        except Exception:
            files = []
        if files:
            try:
                cp = os.path.commonpath([os.path.normpath(f) for f in files])
                if not os.path.isdir(cp):
                    cp = os.path.dirname(cp)
                return os.path.normpath(cp)
            except Exception:
                return os.path.dirname(os.path.normpath(files[0]))
        return ""

    for c in inputs:
        for up in c.getUniversalPaths():
            files = up.getAllFiles()
            mfid = up.getMediaFileInfoData()
            metadata = mfid.getToken("s3metadata")

            root_dir = _get_up_root_dir(up)
            rel_start = os.path.dirname(root_dir) if root_dir else ""

            for f in files:
                if self.isCancelled():
                    return False

                if folder_template:
                    folder = up.evaluateTokensString(folder_template)
                else:
                    folder = ""

                if folder and not folder.endswith('/'):
                    folder += '/'

                norm_f = os.path.normpath(f)

                try:
                    input_root = _find_input_root(os.path.dirname(norm_f))
                except Exception:
                    input_root = None

                try:
                    if input_root:
                        rel = os.path.relpath(norm_f, start=input_root)
                    elif rel_start:
                        rel = os.path.relpath(norm_f, start=rel_start)
                    elif root_dir:
                        rel = os.path.relpath(norm_f, start=root_dir)
                    else:
                        rel = (
                            os.path.basename(os.path.dirname(norm_f))
                            + "/"
                            + os.path.basename(norm_f)
                        )
                except Exception:
                    rel = os.path.basename(norm_f)

                rel = rel.replace("\\", "/").lstrip("/")

                name = f"{folder}{rel}" if folder else rel
                if name.startswith("/"):
                    name = name[1:]

                uploaded = aws.uploadFile(f, name, metadata=metadata)
                if not uploaded:
                    self.addFailedUP(up)
                res = res and uploaded

            if out:
                out.addUniversalPath(up)

    return res
</code>
     <aws_bucket type="string">[s3_path]/[brand]/[campaign]/[job]</aws_bucket>
     <aws_key type="string">AKIAVYGUPQV3QO4PJ2AY</aws_key>
     <aws_secret encrypted="1" type="string">AwUed0F0b3+gjGVG9pO6z57xJKnaCHvCze4GVMMU1MfEKxk/he0PQAJWHJ4hyvFDhK/BYebqqmNfioVfWvGPxr+2WzoComjxgK6b</aws_secret>
    </properties>
    <connections>
     <rankFrom id="a76211e8-13bd-4a04-9c72-e4c68d32aa41" type="input" name="rankFrom" specialType="order"/>
     <rankTo id="ea35e508-0d95-47de-a40b-de9285cb1c22" type="output" name="rankTo" specialType="order"/>
     <in id="8a2f21e4-543c-4a90-adc7-dc6d0e7f3352" type="input" name="in">
      <linkedTo>e0ae5b88-2d5f-4091-8f17-555e27d0956e</linkedTo>
     </in>
     <out id="d33b2e98-66d5-4c99-9d1b-220d2f719ab3" type="output" name="out">
      <linkedTo>d38fac5e-b8cf-4dba-ac6a-e34052694fc4</linkedTo>
     </out>
    </connections>
   </node>
   <node class="iconikS3Sync" type="task">
    <properties>
     <objectName type="string">iconikS3Sync</objectName>
     <color type="color">#3399cc</color>
     <colorByStatus type="bool">false</colorByStatus>
     <pos type="point2">74.5983,-411.744</pos>
     <schemaName type="string">iconiks3sync</schemaName>
     <nameConvention type="CnameConvention"></nameConvention>
     <nodeLocked type="bool">false</nodeLocked>
     <bypassSupported type="bool">true</bypassSupported>
     <bypassEnabled type="bool">false</bypassEnabled>
     <renderFarmSupported type="bool">false</renderFarmSupported>
     <renderFarmSplitContent type="bool">false</renderFarmSplitContent>
     <farmParams type="string"></farmParams>
     <auxCode type="string"></auxCode>
     <renderFarmEnabled type="bool">false</renderFarmEnabled>
     <code type="string"># File lives here: /Users/[username]/SGO AppData/shared/workflowsLibrary/

from Mistika.classes import Cconnector
from Mistika.Qt import QColor

import re
import requests

try:
    import boto3
except ImportError:
    boto3 = None

################################################################################
# Iconik S3 Metadata Sync Mistika Node                                         #
# ====================================                                         #
# E.Spencer - Konsistent Consulting 2025                                       #
# Syncronises files within S3 with files in iconik. If it finds a matchhing    #
# file/folder path then it appends the S3 URI to the iconik metadata and       #
# flags the asset as being archived to S3                                      #
################################################################################

DEFAULT_DOMAIN = "https://app.iconik.io"
DEFAULT_APP_ID = ""
DEFAULT_AUTH_TOKEN = ""
DEFAULT_S3_BUCKET = ""
DEFAULT_S3_KEY = ""
DEFAULT_S3_SECRET = ""


def log_info(self, tag, msg):
    """
    Safe wrapper around self.info so we don't crash on logging.
    """
    try:
        self.info(tag, msg, "")
    except Exception:
        pass


def die(self, msg):
    """
    Emit an error and return False.
    """
    try:
        self.critical("iconikS3Sync:error", msg, "")
    except Exception:
        pass
    return False

def _expand_tokens_from_up(template, context_up):
    """
    Replace [token] placeholders in 'template' with values taken from
    the UniversalPath's param bag (context_up.getParam(token, "")).
    If a token has no value, the original [token] text is left intact.
    """
    import re

    if not template or context_up is None:
        return template

    def repl(match):
        name = (match.group(1) or "").strip()
        if not name:
            return match.group(0)
        try:
            val = context_up.getParam(name, "")
        except Exception:
            val = ""
        return val if val not in (None, "") else match.group(0)

    return re.sub(r"\[([^\]]+)\]", repl, template)

class Iconik:
    """
    REST wrapper for iconik:
    - auth
    - tenant autodetect
    - list collections + contents
    - get assets (with metadata)
    - update asset metadata field via metadata API
    """

    def __init__(self, domain, app_id, auth_token, tenant_id=None):
        self.domain = domain.rstrip("/")
        self.sess = requests.Session()
        self.sess.headers.update(
            {
                "App-ID": app_id,
                "Auth-Token": auth_token,
                "Accept": "application/json",
                "Content-Type": "application/json",
            }
        )
        if tenant_id:
            self.sess.headers["X-Tenant-Id"] = tenant_id


    def set_tenant(self, tenant_id):
        if tenant_id:
            self.sess.headers["X-Tenant-Id"] = tenant_id
        else:
            self.sess.headers.pop("X-Tenant-Id", None)


    def _url(self, path):
        if not path.startswith("/"):
            path = "/" + path
        if "?" in path:
            base, qs = path.split("?", 1)
            if not base.endswith("/"):
                base += "/"
            return f"{self.domain}{base}?{qs}"
        if not path.endswith("/"):
            path += "/"
        return f"{self.domain}{path}"


    def _check(self, r, method, path):
        if not r.ok:
            snip = r.text[:600]
            raise RuntimeError(f"{method} {path} -> {r.status_code}: {snip}")


    def get(self, path, params=None):
        r = self.sess.get(self._url(path), params=params)
        self._check(r, "GET", path)
        return r.json() if r.text else {}


    def post(self, path, payload):
        r = self.sess.post(self._url(path), json=payload)
        self._check(r, "POST", path)
        return r.json() if r.text else {}


    def patch(self, path, payload):
        r = self.sess.patch(self._url(path), json=payload)
        self._check(r, "PATCH", path)
        return r.json() if r.text else {}


    def put(self, path, payload):
        r = self.sess.put(self._url(path), json=payload)
        self._check(r, "PUT", path)
        return r.json() if r.text else {}


    def search_tenant_hint(self):
        body = {
            "query": "",
            "doc_types": ["collections"],
            "per_page": 1,
            "page": 1,
        }
        r = self.post("API/search/v1/search", body)
        objs = r.get("objects") or []
        return objs[0].get("system_domain_id") if objs else None


    def storage_tenant_hint(self):
        r = self.get("API/files/v1/storages")
        for o in r.get("objects", []):
            p = (o.get("settings") or {}).get("path")
            if p and re.fullmatch(r"[0-9a-fA-F\-]{36}", p):
                return p
        return None


    def autodetect_tenant(self):
        try:
            t = self.search_tenant_hint()
            if t:
                return t
        except Exception:
            pass
        try:
            t = self.storage_tenant_hint()
            if t:
                return t
        except Exception:
            pass
        return None


    def list_collections_api(self, page=1, per_page=200,
                             include_deleted=False, include_smart=False):
        params = {"page": page, "per_page": per_page}
        data = self.get("API/assets/v1/collections", params)
        out = []
        for o in data.get("objects", []) or []:
            status = (o.get("status") or "").upper()
            ctype = (o.get("type") or "").upper()
            if not include_deleted and status == "DELETED":
                continue
            if not include_smart and ctype == "SMART":
                continue
            out.append(o)
        pages = data.get("pages") or 1
        return out, pages


    def get_collection_api(self, collection_id):
        return self.get(f"API/assets/v1/collections/{collection_id}")


    def list_collection_contents(self, collection_id, page=1, per_page=200):
        return self.get(
            f"API/assets/v1/collections/{collection_id}/contents",
            params={"page": page, "per_page": per_page},
        )


    def get_asset_api(self, asset_id):
        """
        Get asset INCLUDING metadata, so we can see metadata.s3_uri.
        For your tenant's s3_uri, view-based metadata is used,
        so this 'metadata' block may not contain it.)
        """
        return self.get(f"API/assets/v1/assets/{asset_id}?include=metadata")


    def patch_asset_metadata_field(self, asset_id, field_name, value, label=None, view_id=None):
        """
        Update a metadata field on the asset via metadata API.
            "metadata_values": {
                "s3_uri": {
                    "field_values": [
                        { "value": "s3://bucket/key", "label": "s3://bucket/key" }
                    ]
                }
            }
        The endpoint is:
        /API/metadata/v1/assets/{asset_id}/views/{view_id}/
        """
        if isinstance(value, list):
            value = value[0]

        if not view_id:
            raise RuntimeError(
                f"patch_asset_metadata_field called without view_id for field '{field_name}'"
            )

        field = {"value": value}
        if label:
            field["label"] = value

        body = {
            "metadata_values": {
                field_name: {
                    "field_values": [field]
                }
            }
        }

        return self.put(
            f"API/metadata/v1/assets/{asset_id}/views/{view_id}/",
            body
        )


def assert_uuid(input):
    """
    Check for a valid UUID
    """
    string = (input or "").strip()
    return bool(
        re.fullmatch(
            r"[0-9a-fA-F]{8}-([0-9a-fA-F]{4}-){3}[0-9a-fA-F]{12}",
            string
        )
    )


def _child_collection_by_title(ik, parent_id, title):
    """
    Return child collection id with exact title, or None.
    """
    page, per_page = 1, 200
    while True:
        res = ik.list_collection_contents(parent_id, page=page,
                                          per_page=per_page)
        objs = res.get("objects") or []
        for o in objs:
            ot = (o.get("object_type") or o.get("type") or "").lower()
            if ot != "collections":
                continue
            cid = o.get("object_id") or o.get("id")
            if not cid:
                continue
            meta = ik.get_collection_api(cid)
            t = meta.get("title") or meta.get("name") or ""
            if t == title:
                return cid
        pages = res.get("pages") or 1
        if page >= pages:
            break
        page += 1
    return None


def _root_candidates_by_title(ik, title):
    """
    Return ids of collections with this top-level title
    """
    ids = []
    page = 1
    while True:
        objs, pages = ik.list_collections_api(
            page=page,
            per_page=200,
            include_deleted=False,
            include_smart=False,
        )
        for o in objs:
            t = o.get("title") or o.get("name") or ""
            if t == title and o.get("id"):
                ids.append(o["id"])
        if page >= pages:
            break
        page += 1
    return ids


def _join_paths(a, b):
    """
    Join two path-ish strings with a single '/'.
    """
    a = (a or "").rstrip("/")
    b = (b or "").lstrip("/")
    if not a:
        return b
    if not b:
        return a
    return f"{a}/{b}"


def _build_s3_map(self, s3, bucket):
    """
    Return dict: logical_name -> full S3 key

    logical_name is:
      &lt;full S3 key without file extension>

    e.g. key='Ford/Winter25/.../mickey_trailer_sample.mov'
      -> logical_name='Ford/Winter25/.../mickey_trailer_sample'
    """
    log_info(
        self, "iconikS3Sync:s3", f"Listing S3 bucket='{bucket}' (no prefix)"
    )

    logical_to_key = {}
    paginator = s3.get_paginator("list_objects_v2")
    kwargs = {"Bucket": bucket}

    total = 0
    for page in paginator.paginate(**kwargs):
        contents = page.get("Contents") or []
        for obj in contents:
            key = obj.get("Key") or ""
            if not key or key.endswith("/"):
                continue
            total += 1

            # full path
            rel = key

            # Strip extension
            parts = rel.split("/")
            basename = parts[-1]
            if "." in basename:
                stem = basename.rsplit(".", 1)[0]
            else:
                stem = basename
            if len(parts) > 1:
                logical = "/".join(parts[:-1] + [stem])
            else:
                logical = stem

            prev = logical_to_key.get(logical)
            if prev and prev != key:
                log_info(
                    self,
                    "iconikS3Sync:s3",
                    f"Duplicate logical name '{logical}' for keys '{prev}' and '{key}' (keeping first)"
                )
                continue

            logical_to_key[logical] = key

    log_info(
        self,
        "iconikS3Sync:s3",
        f"Collected {len(logical_to_key)} unique S3 objects (from {total} keys)"
    )
    return logical_to_key


def _walk_collection_tree_build_asset_map(self, ik, base_coll_id, path_prefix, field_name):
    """
    Recursively walk collection tree starting at base_coll_id and build:

      logical_name -> { "asset_id": ..., "existing_value": ... }

    logical_name is: &lt;collection-sub-path>/&lt;asset-title>
    (no extension stripping on title).
    """
    result = {}

    def walk(coll_id, prefix):
        page, per_page = 1, 200
        while True:
            res = ik.list_collection_contents(
                coll_id, page=page, per_page=per_page
            )
            objs = res.get("objects") or []
            for o in objs:
                ot = (o.get("object_type") or o.get("type") or "").lower()
                oid = o.get("object_id") or o.get("id")
                if not oid:
                    continue

                if ot == "collections":
                    meta = ik.get_collection_api(oid)
                    title = (meta.get("title") or meta.get("name") or "").strip()
                    if not title:
                        continue
                    new_prefix = _join_paths(prefix, title)
                    walk(oid, new_prefix)
                elif ot == "assets":
                    try:
                        meta = ik.get_asset_api(oid)
                    except Exception as e:
                        log_info(
                            self,
                            "iconikS3Sync:iconik",
                            f"Failed to get asset {oid}: {e}"
                        )
                        continue
                    title = (meta.get("title") or meta.get("name") or "").strip()
                    if not title:
                        continue
                    logical = _join_paths(prefix, title)
                    existing = (meta.get("metadata") or {}).get(field_name)
                    if logical in result:
                        log_info(
                            self,
                            "iconikS3Sync:iconik",
                            f"Duplicate logical asset path '{logical}' (keeping first)"
                        )
                        continue
                    result[logical] = {
                        "asset_id": oid,
                        "existing_value": existing,
                    }
            pages = res.get("pages") or 1
            if page >= pages:
                break
            page += 1

    walk(base_coll_id, path_prefix or "")
    return result


def _build_global_asset_map(self, ik, field_name):
    """
    Build asset map for ALL root collections and their descendants:

      logical_name -> { "asset_id": ..., "existing_value": ... }

    where logical_name is full path from root collection title down to asset title,
    e.g. 'Ford/Winter25/ABC123_Ford_Winter_2025/COMBINED/mickey_trailer_sample'
    """
    global_map = {}
    page = 1
    while True:
        roots, pages = ik.list_collections_api(
            page=page,
            per_page=200,
            include_deleted=False,
            include_smart=False,
        )
        for o in roots:
            cid = o.get("id")
            if not cid:
                continue
            title = (o.get("title") or o.get("name") or "").strip()
            if not title:
                continue
            log_info(
                self,
                "iconikS3Sync:iconik",
                f"Scanning collection tree under root '{title}' (id='{cid}')"
            )
            sub_map = _walk_collection_tree_build_asset_map(
                self, ik, cid, path_prefix=title, field_name=field_name
            )
            for logical, entry in sub_map.items():
                if logical in global_map:
                    log_info(
                        self,
                        "iconikS3Sync:iconik",
                        f"Global duplicate logical path '{logical}' (keeping first)"
                    )
                    continue
                global_map[logical] = entry
        if page >= pages:
            break
        page += 1

    log_info(
        self,
        "iconikS3Sync:iconik",
        f"Global asset map built with {len(global_map)} entries"
    )
    return global_map


def init(self):
    """
    Define connectors and GUI properties.
    """
    self.setClassName("iconikS3Sync")
    self.color = QColor(0x33, 0x99, 0xCC)

    # Pass-through style node: input -> output
    self.addConnector("input",  Cconnector.CONNECTOR_TYPE_INPUT,  Cconnector.MODE_OPTIONAL)
    self.addConnector("output", Cconnector.CONNECTOR_TYPE_OUTPUT, Cconnector.MODE_OPTIONAL)
    self.setAcceptConnectors(True, "input")

    # Iconik credentials
    self.addProperty("iconik_appId", DEFAULT_APP_ID)
    self.addEncryptedProperty("iconik_authToken", DEFAULT_AUTH_TOKEN)
    self.addProperty("iconik_domain", DEFAULT_DOMAIN)

    # S3 configuration
    self.addProperty("aws_bucket", DEFAULT_S3_BUCKET)
    self.addProperty("s3Prefix", "")  # ignored; full keys used

    self.addProperty("aws_key", DEFAULT_S3_KEY)
    self.addEncryptedProperty("aws_secret", DEFAULT_S3_SECRET)

    # Metadata view id where s3_uri lives
    self.addProperty("metadataViewId", "5522f18c-bbfc-11f0-875b-bae5e7a01d69")

    # Behaviour
    self.addProperty("overwriteExisting", False)

    self.addProperty("objectName", "")

    self.bypassSupported = True
    self.setSupportedTypes(self.NODETYPE_TASK)
    self.setComplexity(80)
    return True



def isReady(self):
    """
    Simple config check only.
    """
    if self.bypassSupported and self.bypassEnabled:
        return True

    ok = True
    app_id = (self.iconik_appId or "").strip()
    auth = (self.iconik_authToken or "").strip()
    dom = (self.iconik_domain or "").strip()
    bucket = (self.aws_bucket or "").strip()

    if not app_id:
        ok = self.critical("iconikS3Sync:iconik_appId", "'iconik_appId' required", "") and ok
    if not auth:
        ok = self.critical("iconikS3Sync:iconik_authToken", "'iconik_authToken' required", "") and ok
    if not dom:
        ok = self.critical("iconikS3Sync:iconik_domain", "'iconik_domain' required", "") and ok
    if not bucket:
        ok = self.critical("iconikS3Sync:aws_bucket", "'aws_bucket' is required", "") and ok

    if boto3 is None:
        ok = self.critical(
            "iconikS3Sync:boto3",
            "boto3 is not available – please install it on this system",
            ""
        ) and ok

    return ok


def process(self):
    """
    Scan S3, match against ALL iconik collection assets by logical path
    (ignoring extensions), and write the S3 key into asset metadata field
    (default 's3_uri') in the configured metadata view.
    """
    log_info(self, "iconikS3Sync:process", "Process started")

    inputs = self.getConnectorsByType(Cconnector.CONNECTOR_TYPE_INPUT)
    output = self.getFirstConnectorByName("output")
    if output:
        output.clearUniversalPaths()

    ups_in = []
    for c in inputs:
        ups_in.extend(c.getUniversalPaths() or [])

    if self.bypassSupported and self.bypassEnabled:
        if output:
            for up in ups_in:
                output.addUniversalPath(up)
        log_info(
            self,
            "iconikS3Sync:process",
            f"Bypass enabled – {len(ups_in)} UP(s) passed through unchanged"
        )
        return True

    if boto3 is None:
        if output:
            for up in ups_in:
                output.addUniversalPath(up)
        return die(self, "boto3 is not available on this system")

    context_up = None
    in_conn = self.getFirstConnectorByName("input")
    if in_conn:
        ups = in_conn.getUniversalPaths() or []
        if ups:
            context_up = ups[0]
            try:
                ctx_path = context_up.getPath()
            except Exception:
                ctx_path = ""
            log_info(
                self,
                "iconikS3Sync:contextUP",
                f"Using first UP from 'input' connector as token context: path='{ctx_path}'"
            )
        else:
            log_info(self, "iconikS3Sync:contextUP", "No UPs on 'input' connector")

    app_id = (self.iconik_appId or "").strip()
    auth = (self.iconik_authToken or "").strip()
    dom = (self.iconik_domain or DEFAULT_DOMAIN).strip()

    # 1) Expand node-level expressions on the property
    try:
        raw_bucket = (self.evaluate(self.aws_bucket) or "").strip()
    except Exception:
        raw_bucket = (self.aws_bucket or "").strip()

    # 2) Expand UP-level tokens using BOTH mechanisms:
    #    - evaluateTokensString (Mistika-style tokens)
    #    - _expand_tokens_from_up (param-bag tokens like [s3_path])
    expanded_bucket = raw_bucket

    if context_up:
        # First: builtin token expansion
        try:
            expanded_bucket = context_up.evaluateTokensString(expanded_bucket)
        except Exception:
            pass

        # Second: our param-bag helper (will replace [token] if present in UP params)
        expanded_bucket = _expand_tokens_from_up(expanded_bucket, context_up)

    # 3) Split into bucket + optional folder/prefix
    partes = (expanded_bucket or "").split("/", 1)
    bucket = (partes[0] or "").strip()

    # 4) Validate bucket – make sure it is not still a [token]
    if not bucket or "[" in bucket or "]" in bucket:
        return die(
            self,
            f"Expanded aws_bucket '{expanded_bucket}' does not contain a valid S3 bucket name. "
            f"Make sure the part before the first '/' is an actual bucket name."
        )

    raw_prefix = (self.s3Prefix or "").strip()
    if raw_prefix:
        log_info(
            self,
            "iconikS3Sync:s3",
            f"NOTE: s3Prefix='{raw_prefix}' is configured but ignored; "
            f"full bucket paths are used for matching."
        )

    aws_key = (self.aws_key or "").strip()
    aws_secret = (self.aws_secret or "").strip()

    # field_name = (self.metadataFieldName or "s3_uri").strip() or "s3_uri"
    field_name = "s3_uri"
    view_id = (self.metadataViewId or "").strip()
    overwrite = bool(self.overwriteExisting)

    log_info(
        self,
        "iconikS3Sync:config",
        f"metadataFieldName='s3_uri', "
        f"metadataViewId='{view_id}', "
        f"overwriteExisting={overwrite}"
    )

    if not view_id:
        return die(self, "metadataViewId must be set (the view where s3_uri lives)")

    try:
        ik = Iconik(dom, app_id, auth, tenant_id=None)
        t = ik.autodetect_tenant()
        if t:
            ik.set_tenant(t)
            log_info(self, "iconikS3Sync:iconik", f"Tenant autodetected: {t}")
        else:
            log_info(self, "iconikS3Sync:iconik", "Tenant autodetection failed or not needed")

        iconik_map = _build_global_asset_map(self, ik, field_name=field_name)

        if aws_key and aws_secret:
            s3 = boto3.client(
                "s3",
                aws_access_key_id=aws_key,
                aws_secret_access_key=aws_secret,
            )
        else:
            s3 = boto3.client("s3")

        s3_map = _build_s3_map(self, s3, bucket)

        matched = 0
        updated = 0
        skipped_existing = 0
        missing_in_iconik = 0

        for logical, s3_key in s3_map.items():
            if self.isCancelled():
                break

            entry = iconik_map.get(logical)
            if not entry:
                missing_in_iconik += 1
                log_info(
                    self,
                    "iconikS3Sync:match",
                    f"S3 logical '{logical}' (key='{s3_key}') has no matching iconik asset"
                )
                continue

            matched += 1
            asset_id = entry["asset_id"]
            existing = entry["existing_value"]

            if existing and not overwrite:
                skipped_existing += 1
                log_info(
                    self,
                    "iconikS3Sync:update",
                    f"Asset {asset_id} already has {field_name}='{existing}', "
                    f"skipping (overwriteExisting=False)"
                )
                continue

            s3_uri = f"s3://{bucket}/{s3_key}"

            log_info(
                self,
                "iconikS3Sync:update",
                f"Asset {asset_id} &lt;- {field_name}=['{s3_uri}'] (logical='{logical}')"
            )

            try:
                # Append the S3 path to the asset
                ik.patch_asset_metadata_field(
                    asset_id,
                    "s3_uri",
                    value=s3_uri,
                    label=s3_uri,
                    view_id=view_id
                )
                # Toggle the "S3_archived" bool field
                ik.patch_asset_metadata_field(
                    asset_id,
                    "s3_archived",
                    value="true",
                    view_id=view_id
                )
                log_info(
                    self,
                    "iconikS3Sync:update",
                    "Updated via metadata API PUT on metadata view "
                    f"{view_id}"
                )
                updated += 1
            except Exception as e:
                log_info(
                    self,
                    "iconikS3Sync:update",
                    f"Failed to update asset {asset_id}: {e}"
                    )

        log_info(
            self,
            "iconikS3Sync:summary",
            f"Matches: {matched}, Updated (incl. dry-run): {updated}, "
            f"Skipped existing: {skipped_existing}, S3-without-asset: {missing_in_iconik}"
        )

    except Exception as e:
        log_info(self, "iconikS3Sync:exception", f"Exception in process(): {e}")
        if output:
            for up in ups_in:
                output.addUniversalPath(up)
        return die(self, f"iconikS3Sync failed: {e}")

    if output:
        for up in ups_in:
            output.addUniversalPath(up)
        log_info(
            self,
            "iconikS3Sync:process",
            f"Process finished, {len(ups_in)} UP(s) sent to 'output' unchanged"
        )

    return True


def onPropertyUpdated(self, name):
    """
    Property-change hook (currently unused).
    """
    try:
        pass
    except AttributeError:
        pass
</code>
     <iconik_appId type="string">8518e744-c53d-11f0-a825-063ff9288a04</iconik_appId>
     <iconik_authToken encrypted="1" type="string">AwWgopTg6w3pOm627krFzl8qBnAooZApJgUEVsFwdwcGp1dpZBH0nzhzoB1YXptbALpwtl+TNeU7+DjmQDEW8mE54DchMBkiJxuJoB7q4Eqf1NHmi/8JQoFTRL07d4LxgTJj68jF3+PSkemDXUe99c7XpOtu+C+yGjxGhYh1liWZa+44tmYaXzM4lTRfIGv0mxkKKwiFTAcmyyfx6BapEe3iHPjgWWxTwFZ56TdlWqcSm9Jnub3O5NeOQy8q/C4Zsqcf6cOc6Mq4mirx/QU=</iconik_authToken>
     <iconik_domain type="string">https://app.iconik.io</iconik_domain>
     <aws_bucket type="string">[s3_path]</aws_bucket>
     <s3Prefix type="string"></s3Prefix>
     <aws_key type="string">AKIAVYGUPQV3QO4PJ2AY</aws_key>
     <aws_secret encrypted="1" type="string">AwX5kKaTiJhHa4KhEXRdKHkWw04975wlKgnhsyTzMyAjzP7YYgrop+Wx+3nGLRakY0gmhgENTYS4bWK4vRZoIVhRvN3lRY8WZ0l8</aws_secret>
     <metadataViewId type="string">5522f18c-bbfc-11f0-875b-bae5e7a01d69</metadataViewId>
     <overwriteExisting type="bool">false</overwriteExisting>
    </properties>
    <connections>
     <rankFrom id="4bd127b8-eee0-48e6-a804-8ab8f1df7414" type="input" name="rankFrom" specialType="order"/>
     <rankTo id="7fe03f42-f60e-4036-b131-144bf65e0047" type="output" name="rankTo" specialType="order">
      <linkedTo>e593e0aa-bc97-4aea-9b16-a36180eec11e</linkedTo>
     </rankTo>
     <input id="d38fac5e-b8cf-4dba-ac6a-e34052694fc4" type="input" name="input">
      <linkedTo>d33b2e98-66d5-4c99-9d1b-220d2f719ab3</linkedTo>
     </input>
     <output id="c64fccdf-342b-4796-b06a-6761219c4e30" type="output" name="output"/>
    </connections>
   </node>
   <node class="Folder" type="input">
    <properties>
     <objectName type="string">INPUT</objectName>
     <color type="color">#808000</color>
     <colorByStatus type="bool">false</colorByStatus>
     <pos type="point2">46.3479,-208.171</pos>
     <schemaName type="string">folder</schemaName>
     <nameConvention type="CnameConvention">[path][baseName][.frame][.ext]</nameConvention>
     <nodeLocked type="bool">false</nodeLocked>
     <bypassSupported type="bool">true</bypassSupported>
     <bypassEnabled type="bool">false</bypassEnabled>
     <renderFarmSupported type="bool">false</renderFarmSupported>
     <renderFarmSplitContent type="bool">false</renderFarmSplitContent>
     <farmParams type="string"></farmParams>
     <auxCode type="string"></auxCode>
     <renderFarmEnabled type="bool">false</renderFarmEnabled>
     <path type="string">/Volumes/konsistent/server/mistika-testing/INPUT/</path>
     <filterMode type="int">1</filterMode>
     <fileNameOnly type="bool">true</fileNameOnly>
     <include type="string"></include>
     <exclude type="string"></exclude>
     <addRoot type="bool">true</addRoot>
     <recursive type="bool">true</recursive>
     <maintainPadding type="bool">true</maintainPadding>
     <alphabeticalOrder type="bool">true</alphabeticalOrder>
     <includeHidden type="bool">false</includeHidden>
     <includeSystem type="bool">false</includeSystem>
    </properties>
    <connections>
     <rankFrom id="e593e0aa-bc97-4aea-9b16-a36180eec11e" type="input" name="rankFrom" specialType="order">
      <linkedTo>7fe03f42-f60e-4036-b131-144bf65e0047</linkedTo>
     </rankFrom>
     <rankTo id="e2bde446-0bd0-41f2-8ade-cde23a05e8a3" type="output" name="rankTo" specialType="order"/>
     <Files id="e937cc7d-72ce-47e1-bb5e-8faa473f5d5f" type="output" name="Files">
      <linkedTo>ecb766d3-3934-43bd-926c-e5deab8acf79</linkedTo>
     </Files>
    </connections>
   </node>
   <node class="Move" type="task">
    <properties>
     <objectName type="string">MoveOriginalFootage</objectName>
     <color type="color">#677688</color>
     <colorByStatus type="bool">false</colorByStatus>
     <pos type="point2">386.67,-208.149</pos>
     <schemaName type="string">move</schemaName>
     <nameConvention type="CnameConvention"></nameConvention>
     <nodeLocked type="bool">false</nodeLocked>
     <bypassSupported type="bool">true</bypassSupported>
     <bypassEnabled type="bool">false</bypassEnabled>
     <renderFarmSupported type="bool">false</renderFarmSupported>
     <renderFarmSplitContent type="bool">false</renderFarmSplitContent>
     <farmParams type="string"></farmParams>
     <auxCode type="string"></auxCode>
     <renderFarmEnabled type="bool">false</renderFarmEnabled>
     <url type="string">/Volumes/konsistent/server/mistika-testing/_done/old_camera_media/</url>
     <autoUpdateNameConvention type="bool">false</autoUpdateNameConvention>
     <overwrite type="int">1</overwrite>
    </properties>
    <connections>
     <rankFrom id="7edd3594-86a9-404d-b18f-9f01049d4855" type="input" name="rankFrom" specialType="order"/>
     <rankTo id="279a1472-55b7-42de-9823-948dd8895701" type="output" name="rankTo" specialType="order"/>
     <File id="ecb766d3-3934-43bd-926c-e5deab8acf79" type="input" name="File">
      <linkedTo>e937cc7d-72ce-47e1-bb5e-8faa473f5d5f</linkedTo>
     </File>
     <Files id="6d05d1c9-31ef-4b6f-91b5-fc6ffd06c3ed" type="output" name="Files"/>
    </connections>
   </node>
  </nodes>
 </workflow>
</transcoder>
